{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab069af",
   "metadata": {},
   "source": [
    "# Homework 2-2 Hessian Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89802509",
   "metadata": {},
   "source": [
    "Imagine we are training a neural network and we are trying to find out whether the model is at **local minima like, saddle point, or none of the above**. We can make our decision by calculating the Hessian matrix.\n",
    "\n",
    "In practice, it is really hard to find a point where the gradient equals zero or all of the eigenvalues in Hessian matrix are greater than zero. In this homework, we make the following two assumptions:\n",
    "1. View gradient norm less than 1e-3 as **gradient equals to zero**.\n",
    "2. If minimum ratio is greater than 0.5 and gradient norm is less than 1e-3, then we assume that the model is at “local minima like”.\n",
    "\n",
    "> Minimum ratio is defined as the proportion of positive eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905fa2b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac6cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32993cd5",
   "metadata": {},
   "source": [
    "## Define NN Model\n",
    "The NN model here is used to fit a single variable math function.\n",
    "$$f(x) = \\frac{\\sin(5\\pi x)}{5\\pi x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eecea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathRegressor(nn.Module):\n",
    "    def __init__(self, num_hidden=128):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c357e1",
   "metadata": {},
   "source": [
    "## Load Pretrained Checkpoints and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4518cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the key from student_id\n",
    "import re\n",
    "\n",
    "key = '6'\n",
    "if re.match('[0-9]', key) is not None:\n",
    "    key = int(key)\n",
    "else:\n",
    "    key = ord(key) % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c18d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint and data corresponding to the key\n",
    "model = MathRegressor()\n",
    "autograd_lib.register(model)\n",
    "\n",
    "data = torch.load('data.pth')[key]\n",
    "model.load_state_dict(data['model'])\n",
    "train, target = data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f4999",
   "metadata": {},
   "source": [
    "## Function to compute gradient norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6ab13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute gradient norm\n",
    "def compute_gradient_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.regressor.children():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) # compute mean of gradient norms\n",
    "\n",
    "    return grad_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c212d1e",
   "metadata": {},
   "source": [
    "## Function to compute minimum ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd80a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code from the official document https://github.com/cybertronai/autograd-lib\n",
    "\n",
    "# helper function to save activations\n",
    "def save_activations(layer, A, _):\n",
    "    '''\n",
    "    A is the input of the layer, we use batch size of 6 here\n",
    "    layer 1: A has size of (6, 1)\n",
    "    layer 2: A has size of (6, 128)\n",
    "    '''\n",
    "    activations[layer] = A\n",
    "\n",
    "# helper function to compute Hessian matrix\n",
    "def compute_hess(layer, _, B):\n",
    "    '''\n",
    "    B is the backprop value of the layer\n",
    "    layer 1: B has size of (6, 128)\n",
    "    layer 2: B ahs size of (6, 1)\n",
    "    '''\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) # do batch-wise outer product\n",
    "\n",
    "    # full Hessian\n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA) # do batch-wise outer product, then sum over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f471453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the minimum ratio\n",
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    # compute Hessian matrix\n",
    "    # save the gradient of each layer\n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    # compute Hessian according to the gradient value stored in the previous step\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    # compute eigenvalues of the Hessian matrix\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        h_eig = torch.symeig(h).eigenvalues # torch.symeig() returns eigenvalues and eigenvectors of a real symmetric matrix\n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio) # compute mean of minimum ratio\n",
    "\n",
    "    return ratio_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103692f9",
   "metadata": {},
   "source": [
    "## Mathematical Derivation\n",
    "\n",
    "Method used here: https://en.wikipedia.org/wiki/Gauss–Newton_algorithm\n",
    "\n",
    "> **Notations** \\\\\n",
    "> $\\mathbf{A}$: the input of the layer. \\\\\n",
    "> $\\mathbf{B}$: the backprop value. \\\\\n",
    "> $\\mathbf{Z}$: the output of the layer. \\\\\n",
    "> $L$: the total loss, mean squared error was used here, $L=e^2$. \\\\\n",
    "> $w$: the weight value.\n",
    "\n",
    "Assume that the input dimension of the layer is $n$, and the output dimension of the layer is $m$.\n",
    "\n",
    "The derivative of the loss is\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial L}{\\partial w}\\right)_{nm} &= \\mathbf{A}_m \\mathbf{B}_n,\n",
    "\\end{align*}\n",
    "\n",
    "which can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial w} &= \\mathbf{B} \\times \\mathbf{A}.\n",
    "\\end{align*}\n",
    "\n",
    "The Hessian can be derived as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H}_{ij}&=\\frac{\\partial^2 L}{\\partial w_i \\partial w_j} \\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i}\\left(\\frac{\\partial L}{\\partial w_j}\\right) \\\\\n",
    "    &= \\frac{\\partial}{\\partial w_i}\\left(\\frac{2e\\partial e}{\\partial w_j}\\right) \\\\\n",
    "    &= 2\\frac{\\partial e}{\\partial w_i}\\frac{\\partial e}{\\partial w_j}+2e\\frac{\\partial^2 e}{\\partial w_j \\partial w_i}.\n",
    "\\end{align*}\n",
    "\n",
    "We neglect the second-order derivative term because the term is relatively small ($e$ is small)\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H}_{ij}\n",
    "    &\\propto \\frac{\\partial e}{\\partial w_i}\\frac{\\partial e}{\\partial w_j},\n",
    "\\end{align*}\n",
    "\n",
    "and as the error $e$ is a constant\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H}_{ij}\n",
    "    &\\propto \\frac{\\partial L}{\\partial w_i}\\frac{\\partial L}{\\partial w_j},\n",
    "\\end{align*}\n",
    "\n",
    "then the full Hessian becomes\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{H} &\\propto (\\mathbf{B}\\times\\mathbf{A})\\times(\\mathbf{B}\\times\\mathbf{A}).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5055a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function to compute gradient norm and minimum ratio\n",
    "def main(model, train, target):\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    gradient_norm = compute_gradient_norm(model, criterion, train, target)\n",
    "    minimum_ratio = compute_minimum_ratio(model, criterion, train, target)\n",
    "\n",
    "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5431e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm: 0.0008722526908968575, minimum ratio: 0.49609375\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # fix random seed\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # reset compute dictionaries\n",
    "    activations = defaultdict(int)\n",
    "    hess = defaultdict(float)\n",
    "\n",
    "    # compute Hessian\n",
    "    main(model, train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfb2dc",
   "metadata": {},
   "source": [
    "从上述结果看来，最后的gradien norm < 1e-3，而minimum ratio < 0.5，表示它是一个saddle point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
